{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.support.wait.WebDriverWait (session=\"cb99a45a-4d61-4e36-ae5e-a82c3c0198ec\")>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "driver_path = \"driver/geckodriver.exe\"\n",
    "page_url = \"https://rmsothebys.com/en/search#/?SortBy=Default&SearchTerm=&Category=All%20Categories&IncludeWithdrawnLots=false&Auction=&OfferStatus=Results&AuctionYear=&Model=Model&Make=Make&FeaturedOnly=false&StillForSaleOnly=false&Collection=All%20Lots&WithoutReserveOnly=false&Day=All%20Days&CategoryTag=All%20Motor%20Vehicles&page=1&pageSize=200&ToYear=NaN&FromYear=NaN\"\n",
    "\n",
    "driver = webdriver.Firefox(executable_path=driver_path)\n",
    "driver.get(page_url)\n",
    "WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = driver.page_source\n",
    "html_soup = BeautifulSoup(html_page)\n",
    "\n",
    "car_containers = html_soup.find_all('div', class_=\"search-result__caption\")\n",
    "search_result = car_containers[0] #0 is the first result of the search, in this case, first car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"search-result__caption\">\n",
       "<h5 class=\"heading-details--bolder ng-binding\">\n",
       "</h5>\n",
       "<p class=\"heading-subtitle--bold ng-binding\">2006 Ford GT </p>\n",
       "<p>\n",
       "<span class=\"heading-subtitle--bold ng-binding\">Sold For $310,500</span><br/>\n",
       "<span class=\"heading-subtitle--bold ng-binding ng-hide\" ng-show=\"item.PreSaleEstimate.length &gt; 0\"></span><br/>\n",
       "<span class=\"heading-details--bolder ng-binding\">Offered without reserve</span><br/>\n",
       "<span class=\"heading-details--bolder ng-binding ng-hide\" ng-show=\"item.StyleClass == 'RMOnlineAuctions' &amp;&amp; item.CurrentBid\"><br/></span>\n",
       "<span class=\"heading-details--bolder ng-binding ng-hide\" ng-show=\"item.StyleClass == 'RMOnlineAuctions' &amp;&amp; item.TimeLeft\"><br/></span>\n",
       "</p>\n",
       "<span class=\"heading-details--bolder search-result__bottom-left ng-binding\">RM | ONLINE ONLY</span>\n",
       "</div>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of how scraping works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access just the fields we need using find_all method and restricting the result to the order of occurence of the HTML tag within the search result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_info = search_result.find_all('p')[0]\n",
    "price = search_result.find_all('span')[0] #price can be found in the first span, on the 0th position so to speak\n",
    "auction_type = search_result.find_all('span')[-1] #auction type can be found in the last span found\n",
    "auction_location = search_result.find_all('h5')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to obtain values that are of interest to us, but we are stuck with HTML code. To get rid of it, we'll use regular expressions. The following expression uses: \n",
    "* lookahead to determine characters preceding our pattern,\n",
    "* a capture group to make accessing our pattern easier,\n",
    "* lookbehind to determine characters following our pattern.\n",
    "\n",
    "Because re.search expects it's arguments to be a string, we need to convert our search results to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(?<=\">)\\s*(.*)\\s*(?=</)'\n",
    "\n",
    "car_re = re.search(pattern, str(car_info), re.IGNORECASE).group(1)\n",
    "price_re = re.search(pattern, str(price), re.IGNORECASE).group(1)\n",
    "auct_type_re = re.search(pattern, str(auction_type), re.IGNORECASE).group(1)\n",
    "auct_loc_re = re.search(pattern, str(auction_location), re.IGNORECASE).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 Ford GT  Sold For $310,500 RM | ONLINE ONLY \n"
     ]
    }
   ],
   "source": [
    "print(car_re, price_re, auct_type_re, auct_loc_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are two ways to go about this:\n",
    "1. Get the search results, append them to one of the tables, construct a DataFrame object, change the types to String and then apply regular expressions to a DataFrame using vectorised methods or\n",
    "2. Get the search results, change the types to String, apply regular expressions, then append them to one ofthe tables and construct a DataFrame object.\n",
    "\n",
    "Common sense suggests, that the first approach would be better, but let's keep in mind, that the raw scrape results are long HTML tags, making everything really hard to read and spotting potential errors even harder. Measuring performance differences between the two approaches is currently not the scope of the project, but can be easily done using e.g. timeit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_current_page():\n",
    "    el_per_page = \n",
    "    \n",
    "def scrape_multiple_pages(number_of_pages, elements_per_page):\n",
    "    for page in number_of_pages:\n",
    "        scrape_current_page(elemets_per_page)\n",
    "        \n",
    "# applying the regex last, on a finished DataFrame object, using vectorised methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you're finished with scraping, close the webdriver\n",
    "# wd.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
