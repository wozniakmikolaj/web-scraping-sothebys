{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating the scraping process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are two ways to go about this:\n",
    "1. Get the search results, append them to one of the tables, construct a DataFrame object, change the types to String and then apply regular expressions to a DataFrame using vectorised methods or\n",
    "2. Get the search results, change the types to String, apply regular expressions, then append them to one ofthe tables and construct a DataFrame object.\n",
    "\n",
    "Common sense suggests, that the first approach would be better, but let's keep in mind, that the raw scrape results are long HTML tags, making everything really hard to read and spotting potential errors even harder. Measuring performance differences between the two approaches is currently not the scope of the project, but can be easily done using with for example timeit module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "export_path = \"data/sothebys_scraped.csv\"\n",
    "driver_path = \"driver/geckodriver.exe\"\n",
    "page_url = \"https://rmsothebys.com/en/search#/?SortBy=Default&SearchTerm=&Category=All%20Categories&IncludeWithdrawnLots=false&Auction=&OfferStatus=Results&AuctionYear=&Model=Model&Make=Make&FeaturedOnly=false&StillForSaleOnly=false&Collection=All%20Lots&WithoutReserveOnly=false&Day=All%20Days&CategoryTag=All%20Motor%20Vehicles&page=1&pageSize=200&ToYear=NaN&FromYear=NaN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tables that will hold our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_info_table = []\n",
    "price_table = []\n",
    "auction_type_table = []\n",
    "auction_location_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating the process of scraping each of the web pages, for more detail and step by step guide to scraping elements of the web page, go [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_pages = 5\n",
    "pattern = r'(?<=\">)\\s*(.*)\\s*(?=</)'\n",
    "\n",
    "for page_number in range(num_of_pages):\n",
    "    page = \"https://rmsothebys.com/en/search#/?SortBy=Default&SearchTerm=&Category=All%20Categories&IncludeWithdrawnLots=false&Auction=&OfferStatus=Results&AuctionYear=&Model=Model&Make=Make&FeaturedOnly=false&StillForSaleOnly=false&Collection=All%20Lots&WithoutReserveOnly=false&Day=All%20Days&CategoryTag=All%20Motor%20Vehicles&page={}&pageSize=200&ToYear=NaN&FromYear=NaN\".format(page_number+1)\n",
    "    driver = webdriver.Firefox(executable_path=driver_path)\n",
    "    driver.get(page)\n",
    "    html_page = BeautifulSoup(driver.page_source)\n",
    "    result_container = html_page.find_all('div', class_=\"search-result__caption\")\n",
    "    \n",
    "    for result_number in range(len(result_container)):\n",
    "        search_result = result_container[result_number] #0 is the first result of the search, in this case, first car\n",
    "                \n",
    "        car_info = search_result.find_all('p')[0]\n",
    "        price = search_result.find_all('span')[0] #price can be found in the first span, on the 0th position so to speak\n",
    "        auction_type = search_result.find_all('span')[-1] #auction type can be found in the last span found\n",
    "        auction_location = search_result.find_all('h5')[0]\n",
    "        \n",
    "        car_re = re.search(pattern, str(car_info), re.IGNORECASE).group(1)\n",
    "        price_re = re.search(pattern, str(price), re.IGNORECASE).group(1)\n",
    "        auct_type_re = re.search(pattern, str(auction_type), re.IGNORECASE).group(1)\n",
    "        auct_loc_re = re.search(pattern, str(auction_location), re.IGNORECASE).group(1)\n",
    "        \n",
    "        car_info_table.append(car_re)\n",
    "        price_table.append(price_re)\n",
    "        auction_type_table.append(auct_type_re)\n",
    "        auction_location_table.append(auct_loc_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_info_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the tables as a DataFrame object, preparing for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sothebys_df = pd.DataFrame({\"car_info\": car_info_table, \"price\": price_table, \n",
    "                            \"auction_type\": auction_type_table, \"auction_location\": auction_location_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to save the data as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported = sothebys_df.to_csv (export_path, index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We have successfully scraped nearly 200.000 data points, from over 1100 web pages. Our dataset combining all the data, has almost 50.000 rows across 4 columns, making it ready for data cleaning process. To see the scraping process in detail, click [here](). To see the data cleaning process, click [here]()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
